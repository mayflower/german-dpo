Optimizing LLMs with Direct Preference Optimization (DPO): An introduction and application with real-world examples.

1. Introduction and Background

Unsupervised language models (LMs) can learn broad knowledge but are difficult to control. Current approaches involve human feedback and reinforcement learning, which are complex and unstable. In this blog post, we cover Direct Preference Optimization (DPO), a simpler and efficient method for aligning LMs with human preferences.
DPO eliminates the need for extensive tuning and LM sampling by using a classification loss for reinforcement learning from human feedback. Results show that DPO outperforms existing methods in sentiment control with comparable response quality. With DPO, we can control LMs effectively without compromising performance or requiring extensive resources.

2. Motivation and Benefits

To tackle the challenges faced when implementing reinforcement learning algorithms on large-scale problems like fine-tuning language models, DPO is developed as a simple approach for policy optimization using preferences directly [3]. Unlike previous methods in RLHF (Reinforcement Learning from Human Feedback), which involve learning a reward and then optimizing it through reinforcement learning, DPO exploits a specific parameterization of the reward model. This parameterization enables DPO to extract the optimal policy for it in a closed form, eliminating the need for an RL training loop. DPO involves utilizing an analytical mapping that connects reward functions with optimal policies. By doing so, it converts a loss function focused on reward functions into a loss function centered around policies. This transformation approach avoids constructing an explicit and independent reward model, while still optimizing based on established models of human preferences such as the Bradley-Terry model [2]. Essentially, the policy network encompasses both the language model and the implicit reward.

In summary, DPO is a simplified approach for implementing reinforcement learning algorithms on large-scale problems like language models. It eliminates the need for an RL training loop by directly optimizing the policy based on human preferences, without constructing an explicit reward model.

3. Foundations to Direct Preference Optimization (DPO)

The DPO process was initially published in the paper "Direct preference optimization: Your language model is secretly a reward model." [2] in 2023. It introduced the new optimization method for LLMs as an answer of weaknesses regarding the reinforcement learning from human feedback (RLHF) method. They focus on a simple approach for policy optimization using preferences directly.

<Input figure from paper here>

Typically RLHF uses human feedback to first fit a reward model to a dataset of prompts and human preferences over pairs of responses and then use RL to find a policy that maximizes the learned reward. DPO directly optimizes for the policy best satisfying the preferences with a simple classification object, fitting an implicit reward model whose corresponding optimal policy can be extracted in closed form.

DPO can directly finetune the LM without training an explicit reward model upfront. The data itself, which contains the information as pairs of chosen and rejected answers, is used to leverage the model behavior. The training procedure used an implicit model to find the policy that drives the optimization.

4. Showing steps how it works

To make it more transparent, in this section, we show the DPO pipeline that is typically used to run a direct preference optimization. In detail, we cover the

- base model,
- how to make the DPO training dataset,
- the DPO training itself,
- and the benchmarking procedure to test your optimized model.

We explain the steps with the help of real-world examples to make it more transparent what is happening and you can use it directly in optimizing your own LLM's. We also linked the model and code repos that we used in our DPO pipeline, which you can use to dive deeper or directly use in you cases. Let's start now by sketching the process and starting with the base model.

Base model

We started with the Mistral-7B-v0.1 model (link: https://huggingface.co/mistralai/Mistral-7B-v0.1), which is a large language model trained on a diverse range of internet text. It is a powerful base model that we used to optimize with DPO.

The model is a pre-trained generative text model with 7 billion parameters. It's a transformer model, with the following architecture choices, grouped-query attention, sliding-window attention, and byte-fallback BPE tokenizer.

DPO training dataset preparation

The second step addresses the data preparation, which is used to directly optimize our base model. It generates the sample completions to contrast between the rejected/lower ranked and chosen/preferred ranked answers to a certain user prompt/question. The semantical differences between the texts create the reward function which is used to optimize the base model. Our DPO training data set is available here: https://huggingface.co/datasets/mayflowergmbh/intel_orca_dpo_pairs_de. It is a German dataset, as we focus on optimizing the base LLM to handle German prompts and responses, but you can also use the original DPO dataset for English texts as well, which you find here: https://huggingface.co/datasets/Intel/orca_dpo_pairs.

Our dataset contains the following columns:

text instruction input output

Sie sind ein KI-Assistent. Sie erhalten eine Aufgabe. Sie müssen eine detaillierte und lange Antwort generieren. Bitte beantworten Sie folgende Frage: Ich möchte die Fähigkeit der Schülerinnen und Schüler testen, eine Passage zu lesen und Fragen dazu zu beantworten. Könnten Sie sich bitte eine gute Frage für die Passage ausdenken: "Im Jahr 1901 war die Föderation von Australien der Prozess, durch den die sechs separaten britischen selbstverwalteten Kolonien New South Wales, Queensland, South Australia, Tasmanien, Victoria und Western Australia eine Nation bildeten. Sie behielten die Regierungssysteme bei, die sie als separate Kolonien entwickelt hatten, hatten aber auch eine föderale Regierung, die für Angelegenheiten verantwortlich war, die die ganze Nation betrafen. Als die Verfassung Australiens in Kraft trat, wurden die Kolonien kollektiv zu Staaten des Commonwealth of Australia. ... [Anm. Author: Text gekürzt]

Sie sind ein KI-Assistent. Sie erhalten eine Aufgabe. Sie müssen eine detaillierte und lange Antwort generieren.

Bitte beantworten Sie folgende Frage: Ich möchte die Fähigkeit der Schülerinnen und Schüler testen, eine Passage zu lesen und Fragen dazu zu beantworten. Könnten Sie sich bitte eine gute Frage für die Passage ausdenken: "Im Jahr 1901 war die Föderation von Australien der Prozess, durch den die sechs separaten britischen selbstverwalteten Kolonien New South Wales, Queensland, South Australia, Tasmanien, Victoria und Western Australia eine Nation bildeten. ... [Anm. Author: Text gekürzt]

["Unter Berücksichtigung der Passage, diskutieren Sie die primären Beweggründe und Ergebnisse der Föderation von Australien im Jahr 1901, einschließlich der Rollen und Verantwortlichkeiten der Bundesregierung, sowie der andauernden Regierungsstrukturen der individuellen beteiligten Staaten. Betrachten Sie dabei zusätzlich die Fähigkeit der Schülerinnen und Schüler, basierend auf dem gelesenen Text Fragen zu verstehen und zu beantworten. Analysieren Sie, warum die neu formierten Staaten ihre eigenen Regierungssysteme beibehalten haben, als sie sich zum Commonwealth of Australia zusammenschlossen, und reflektieren Sie über die Gründe, warum die föderale Regierung für Angelegenheiten zuständig war, die die gesamte Nation betrafen. ... [Anm. Author: Text gekürzt]"

"Welche Bedeutung hatte die Gründung der Föderation von Australien im Jahr 1901?"]

DPO training
https://github.com/mayflower/LLaMA-Factory-de_old

Fine-tuned DPO
DiscoPhoenix-7B, https://huggingface.co/mayflowergmbh/DiscoPhoenix-7B-dpo

Benchmarking
xxx
Midigate Risks/Pitfalls
xxx
5. Insights and Best-Practices
Pro and Cons
6. Summary and Future Directions
xxx
Links & References
[1] Argilla. (2024). Ultrafeedback-binarized-preferences dataset. https://huggingface.co/datasets/argilla/ultrafeedback-binarized-preferences. Last accessed: 13. February, 2024.
[2] R. A. Bradley and M. E. Terry. (1952) Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324–345. doi: https://doi.org/10.2307/2334029.
[3] Rafailov, R., Sharma, A., Mitchell, E., Ermon, S., Manning, C. D., & Finn, C. (2023). Direct preference optimization: Your language model is secretly a reward model. arXiv preprint arXiv:2305.18290.